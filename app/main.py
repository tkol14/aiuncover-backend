from fastapi import FastAPI, UploadFile, File, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from PIL import Image, ImageOps, ImageChops
import io, re, json
import numpy as np
import exifread
import os
import itertools
from collections import Counter
# import cv2 # –í–ò–î–ê–õ–ï–ù–û: –¢–µ–ø–µ—Ä –Ω–µ –ø–æ—Ç—Ä—ñ–±–Ω–æ

# --- –ù–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è FastAPI ---

app = FastAPI(title="AIUncover API", version="1.4.1 - No CV2", debug=os.environ.get("DEBUG", "False").lower() == "true")

ALLOWED_ORIGINS = [
    "https://aiuncover.net",
    "https://www.aiuncover.net",
    "https://aiuncover-backend-production.up.railway.app",
    "https://api.aiuncover.net",
    "http://localhost:3000"
]

app.add_middleware(
    CORSMiddleware,
    allow_origins=ALLOWED_ORIGINS,
    allow_credentials=True,
    allow_methods=["GET", "POST", "OPTIONS"],
    allow_headers=["*"],
)

class AnalyzeResponse(BaseModel):
    prob_ai: float
    explanations: list[str]
    checks: dict

# --- –ö–æ–Ω—Å—Ç–∞–Ω—Ç–∏ (–±–µ–∑ –∑–º—ñ–Ω) ---
AI_TOOL_HINTS = [
    "stable diffusion", "stablediffusion", "sdxl", "novelai",
    "midjourney", "dall-e", "dalle", "leonardo", "flux", "playground",
    "firefly", "adobe firefly", "cf ai", "canva ai", "ideogram", "pollinations",
    "dreamstudio", "getimg.ai", "artbreeder", "pika labs", "runway", "gen-1", "gen-2",
    "aigenerated", "ai generated"
]

COMMON_AI_SIZES = {256, 384, 448, 512, 576, 640, 704, 768, 896, 960, 1024, 1152, 1280, 1536, 1792, 2048}
UNCOMMON_RATIOS = {
    (1, 1), (4, 3), (3, 4), (16, 9), (9, 16), (3, 2), (2, 3), (5, 4), (4, 5)
}

# --- –£—Ç–∏–ª—ñ—Ç–∏ (–±–µ–∑ –∑–º—ñ–Ω) ---
def gcd(a, b):
    while b:
        a, b = b, a % b
    return a

def load_image(raw: bytes):
    bio = io.BytesIO(raw)
    img = Image.open(bio)
    img.load()
    return img

# --- –ú–æ–¥—É–ª—ñ –ü–µ—Ä–µ–≤—ñ—Ä–æ–∫ (–∑–º—ñ–Ω–∏ –ª–∏—à–µ –≤ noise_analysis) ---

def read_exif_hints(raw: bytes):
    # (–±–µ–∑ –∑–º—ñ–Ω)
    hints = []
    has_exif = False
    ai_tool_found = False
    try:
        tags = exifread.process_file(io.BytesIO(raw), details=False)
        if tags:
            has_exif = True
            text = " ".join([str(v) for v in tags.values()]).lower()
            if any(h in text for h in AI_TOOL_HINTS):
                hints.append("üö© EXIF/XMP –º—ñ—Å—Ç–∏—Ç—å –∑–≥–∞–¥–∫–∏ –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä—ñ–≤ –®–Ü")
                ai_tool_found = True
        else:
            hints.append("üìù EXIF –≤—ñ–¥—Å—É—Ç–Ω—ñ–π (—á–∞—Å—Ç–∏–π –≤–∏–ø–∞–¥–æ–∫ –¥–ª—è –®–Ü)")
    except Exception:
        hints.append("‚ùå –ù–µ –≤–¥–∞–ª–æ—Å—è –ø—Ä–æ—á–∏—Ç–∞—Ç–∏ EXIF")
    return has_exif, ai_tool_found, hints

def png_metadata_check(img: Image.Image, fmt: str):
    # (–±–µ–∑ –∑–º—ñ–Ω)
    reasons = []
    ai_prompt_found = False
    if fmt == "PNG":
        try:
            metadata = img.info
            text = json.dumps(metadata).lower()
            
            if any(h in text for h in AI_TOOL_HINTS) or "prompt" in text or "parameters" in text:
                reasons.append("üö© PNG –º–µ—Ç–∞–¥–∞–Ω—ñ (tEXt) –º—ñ—Å—Ç—è—Ç—å –ø—ñ–¥–∫–∞–∑–∫–∏/–ø—Ä–æ–º–ø—Ç –®–Ü")
                ai_prompt_found = True
            elif "software" in metadata and any(h in metadata["software"].lower() for h in AI_TOOL_HINTS):
                reasons.append(f"üö© PNG –º–µ—Ç–∞–¥–∞–Ω—ñ: —É –ø–æ–ª—ñ 'Software' –∑–Ω–∞–π–¥–µ–Ω–æ –®–Ü-—ñ–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç")
        except Exception:
            pass
    return ai_prompt_found, reasons

def size_checks(img: Image.Image):
    # (–±–µ–∑ –∑–º—ñ–Ω)
    w, h = img.size
    reasons = []
    flags = {}
    
    mul_64 = (w % 64 == 0) and (h % 64 == 0)
    mul_8 = (w % 8 == 0) and (h % 8 == 0)
    square_common = (w == h) and (w in COMMON_AI_SIZES)
    
    g = gcd(w, h)
    ratio = (w // g, h // g)
    is_uncommon_ratio = ratio not in UNCOMMON_RATIOS and max(w, h) >= 512
    
    if mul_64:
        reasons.append("üìè –†–æ–∑–º—ñ—Ä–∏ –∫—Ä–∞—Ç–Ω—ñ 64 (—Å–∏–ª—å–Ω–∞ –æ–∑–Ω–∞–∫–∞ Stable Diffusion, SDXL)")
    elif mul_8:
        reasons.append("üìè –†–æ–∑–º—ñ—Ä–∏ –∫—Ä–∞—Ç–Ω—ñ 8 (—Å–ª–∞–±–∫–∞ –æ–∑–Ω–∞–∫–∞)")
        
    if square_common:
        reasons.append(f"üñºÔ∏è –ö–≤–∞–¥—Ä–∞—Ç–Ω–∏–π —Ñ–æ—Ä–º–∞—Ç {w}√ó{h}, —Ç–∏–ø–æ–≤–∏–π –¥–ª—è –®–Ü-–º–æ–¥–µ–ª–µ–π")
        
    if is_uncommon_ratio and (w not in COMMON_AI_SIZES and h not in COMMON_AI_SIZES):
        reasons.append(f"üìê –ù–µ—Ç–∏–ø–æ–≤–∞ —á–∏ –¥—É–∂–µ –≤–∏—Å–æ–∫–∞/–Ω–∏–∑—å–∫–∞ –ø—Ä–æ–ø–æ—Ä—Ü—ñ—è {ratio[0]}:{ratio[1]} –¥–ª—è —Ñ–æ—Ç–æ")
        
    flags["mul64"] = mul_64
    flags["square_common"] = square_common
    flags["is_uncommon_ratio"] = is_uncommon_ratio
    flags["size"] = (w, h)
    return flags, reasons

def alpha_channel_weird(img: Image.Image, fmt: str):
    # (–±–µ–∑ –∑–º—ñ–Ω)
    has_alpha = img.mode in ("LA", "RGBA", "P", "PA")
    if fmt == "JPEG" and has_alpha:
        return True, "‚ö†Ô∏è JPEG –∑ –∞–ª—å—Ñ–∞-–∫–∞–Ω–∞–ª–æ–º ‚Äî –Ω–µ—Ç–∏–ø–æ–≤–æ –¥–ª—è —Ñ–æ—Ç–æ–≥—Ä–∞—Ñ—ñ—ó"
    return False, None

def high_freq_heuristic(img: Image.Image):
    # (–±–µ–∑ –∑–º—ñ–Ω)
    gray = ImageOps.grayscale(img)
    gray_small = gray.resize((256, int(256 * gray.height / gray.width)), Image.Resampling.LANCZOS) if gray.width > 256 else gray
    arr = np.asarray(gray_small, dtype=np.float32) / 255.0

    k = np.array([[0, 1, 0],
                  [1,-4, 1],
                  [0, 1, 0]], dtype=np.float32)
    
    # –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ –∑–≥–æ—Ä—Ç–∫—É NumPy
    pad_width = 1
    arr_padded = np.pad(arr, pad_width, mode='edge')
    # –°–ø—Ä–æ—â–µ–Ω–∞ –∑–≥–æ—Ä—Ç–∫–∞ –¥–ª—è —É–Ω–∏–∫–Ω–µ–Ω–Ω—è –ø—Ä–æ–±–ª–µ–º –∑ SciPy:
    lap = np.abs(np.array([[np.sum(arr_padded[i:i+3, j:j+3] * k) for j in range(arr.shape[1])] for i in range(arr.shape[0])]))

    lap_var = float(np.var(lap))

    f = np.fft.fft2(arr)
    fshift = np.fft.fftshift(f)
    mag = np.abs(fshift)
    h, w = mag.shape
    cy, cx = h//2, w//2
    r = min(cy, cx)
    
    low = mag[cy-r//4:cy+r//4, cx-r//4:cx+r//4].sum() + 1e-6
    high = mag.sum() - low
    high_ratio = float(high / (high + low))

    ai_like = (lap_var < 0.0003 and high_ratio > 0.55)
    expl = f"üî¨ –õ–æ–∫–∞–ª—å–Ω–∞ —Ä—ñ–∑–∫—ñ—Å—Ç—å (var Laplacian={lap_var:.4f}), —á–∞—Å—Ç–æ—Ç–Ω–µ –Ω–∞—Å–∏—á–µ–Ω–Ω—è={high_ratio:.2f}"
    
    if ai_like:
        expl = "üß† –°–∏–Ω—Ç–µ—Ç–∏—á–Ω–∞ —Ç–µ–∫—Å—Ç—É—Ä–∞: –¥—É–∂–µ –Ω–∏–∑—å–∫–∞ –ª–æ–∫–∞–ª—å–Ω–∞ —Ä—ñ–∑–∫—ñ—Å—Ç—å (Laplacian Var) —Ç–∞ –≤–∏—Å–æ–∫–∞ —á–∞—Å—Ç–æ—Ç–Ω–∞ –µ–Ω–µ—Ä–≥—ñ—è ('–ø–ª–∞—Å—Ç–º–∞—Å–æ–≤—ñ—Å—Ç—å')"
        
    return ai_like, expl, {"lap_var": lap_var, "high_ratio": high_ratio}

def jpeg_quant_hint(img: Image.Image, fmt: str):
    # (–±–µ–∑ –∑–º—ñ–Ω)
    try:
        if fmt == "JPEG" and hasattr(img, "quantization") and img.quantization:
            q = img.quantization
            if len(q) <= 2 and all(len(v) == 64 for v in q.values()):
                return False, None
            else:
                return True, "üí° –ù–µ—Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ñ JPEG quantization tables (—Å–ª–∞–±–∫–∞ –æ–∑–Ω–∞–∫–∞, –º–æ–∂–µ –±—É—Ç–∏ —Ä–µ-–∫–æ–º–ø—Ä–µ—Å—ñ—è)"
    except Exception:
        pass
    return False, None

def jpeg_artifact_hint(img: Image.Image, fmt: str):
    # (–±–µ–∑ –∑–º—ñ–Ω)
    if fmt == "JPEG" and img.mode in ("RGB", "L"):
        try:
            temp_io = io.BytesIO()
            img.save(temp_io, format="JPEG", quality=95)
            re_compressed = Image.open(temp_io)
            
            diff = ImageChops.difference(img, re_compressed)
            diff_gray = ImageOps.grayscale(diff)
            arr = np.asarray(diff_gray, dtype=np.float32)
            
            std_dev = np.std(arr)
            mean_abs_dev = np.mean(np.abs(arr))
            
            is_low_artifact = (std_dev < 10.0 and mean_abs_dev < 5.0)
            
            if is_low_artifact:
                return True, f"üñºÔ∏è –î—É–∂–µ –Ω–∏–∑—å–∫–∏–π —Ä—ñ–≤–µ–Ω—å –∞—Ä—Ç–µ—Ñ–∞–∫—Ç—ñ–≤ (STD={std_dev:.2f}) –ø—ñ—Å–ª—è —Ä–µ-–∫–æ–º–ø—Ä–µ—Å—ñ—ó (–º–æ–∂–µ –±—É—Ç–∏ –ø–µ—Ä—à–µ –∑–±–µ—Ä–µ–∂–µ–Ω–Ω—è –®–Ü)"
                
        except Exception:
            pass
    return False, None

def noise_analysis(img: Image.Image):
    """
    –û–Ω–æ–≤–ª–µ–Ω–∏–π –ê–Ω–∞–ª—ñ–∑ —à—É–º—É: –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î –ª–∏—à–µ PIL/NumPy.
    –í–∏–º—ñ—Ä—é—î —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–µ –≤—ñ–¥—Ö–∏–ª–µ–Ω–Ω—è –≤–∏—Å–æ–∫–æ—á–∞—Å—Ç–æ—Ç–Ω–æ—ó —Å–∫–ª–∞–¥–æ–≤–æ—ó (—à—É–º—É) –∑–∞ –¥–æ–ø–æ–º–æ–≥–æ—é –õ–∞–ø–ª–∞—Å—ñ–∞–Ω–∞.
    """
    
    # –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ –õ–∞–ø–ª–∞—Å—ñ–∞–Ω –¥–ª—è –≤–∏–¥—ñ–ª–µ–Ω–Ω—è —à—É–º—É (–¥–µ—Ç–∞–ª–µ–π)
    try:
        gray = ImageOps.grayscale(img)
        arr = np.asarray(gray, dtype=np.float32)
        
        # –õ–∞–ø–ª–∞—Å—ñ–∞–Ω —á–µ—Ä–µ–∑ —è–¥—Ä–æ (—è–∫ —É high_freq_heuristic, –∞–ª–µ –Ω–∞ –ø–æ–≤–Ω–æ–º—É —Ä–æ–∑–º—ñ—Ä—ñ –¥–ª—è –∫—Ä–∞—â–æ—ó —Ç–æ—á–Ω–æ—Å—Ç—ñ)
        k = np.array([[0, 1, 0],
                      [1,-4, 1],
                      [0, 1, 0]], dtype=np.float32)
        
        pad_width = 1
        arr_padded = np.pad(arr, pad_width, mode='edge')
        
        # –û–±—á–∏—Å–ª–µ–Ω–Ω—è Lapalacian –¥–ª—è –≤—Å—å–æ–≥–æ –∑–æ–±—Ä–∞–∂–µ–Ω–Ω—è
        laplacian_map = np.abs(np.array([[np.sum(arr_padded[i:i+3, j:j+3] * k) 
                                          for j in range(arr.shape[1])] 
                                         for i in range(arr.shape[0])]))
        
        # –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–µ –≤—ñ–¥—Ö–∏–ª–µ–Ω–Ω—è –õ–∞–ø–ª–∞—Å—ñ–∞–Ω–∞ —î –≥–∞—Ä–Ω–∏–º –ø—Ä–æ–∫—Å—ñ –¥–ª—è –∑–∞–≥–∞–ª—å–Ω–æ—ó —Ç–µ–∫—Å—Ç—É—Ä–∏/—à—É–º—É
        noise_std = np.std(laplacian_map)
    except Exception:
        return False, "‚ùå –ü–æ–º–∏–ª–∫–∞ –≤ –∞–Ω–∞–ª—ñ–∑—ñ —à—É–º—É (NumPy/PIL)", {"noise_std": -1.0}

    # –ï–º–ø—ñ—Ä–∏—á–Ω–∞ –µ–≤—Ä–∏—Å—Ç–∏–∫–∞ –¥–ª—è NumPy Lap Std (–º–µ–Ω—à–µ 0.005 —á–∞—Å—Ç–æ –≤–∫–∞–∑—É—î –Ω–∞ –Ω–∞–¥–º—ñ—Ä–Ω—É –≥–ª–∞–¥–∫—ñ—Å—Ç—å)
    ai_too_smooth = noise_std < 0.004 
    
    if ai_too_smooth:
        expl = f"‚ú® –ù–∞–¥—Ç–æ –≥–ª–∞–¥–∫–µ –∑–æ–±—Ä–∞–∂–µ–Ω–Ω—è: –ù–∞–¥–∑–≤–∏—á–∞–π–Ω–æ –Ω–∏–∑—å–∫–∏–π —Ä—ñ–≤–µ–Ω—å —à—É–º—É/—Ç–µ–∫—Å—Ç—É—Ä–∏ (Laplacian STD={noise_std:.4f}) ‚Äî –º–æ–∂–µ –±—É—Ç–∏ –æ–∑–Ω–∞–∫–æ—é –®–Ü-–≥–µ–Ω–µ—Ä–∞—Ü—ñ—ó –∞–±–æ –∞–≥—Ä–µ—Å–∏–≤–Ω–æ–≥–æ Denoising."
        return True, expl, {"noise_std": float(noise_std)}
        
    return False, f"üî¨ –ê–Ω–∞–ª—ñ–∑ —à—É–º—É: Laplacian STD={noise_std:.4f} (–≤ –º–µ–∂–∞—Ö –Ω–æ—Ä–º–∏)", {"noise_std": float(noise_std)}


def color_statistic_check(img: Image.Image):
    # (–±–µ–∑ –∑–º—ñ–Ω)
    try:
        hsv_img = img.convert("HSV")
        hsv_arr = np.asarray(hsv_img, dtype=np.float32) / 255.0
    except Exception:
        return False, "‚ùå –ü–æ–º–∏–ª–∫–∞ –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü—ñ—ó –≤ HSV", {}
        
    S = hsv_arr[:, :, 1]
    mean_S = np.mean(S)
    
    is_oversaturated = mean_S > 0.60
    
    high_S_count = np.sum(S > 0.95)
    total_pixels = S.size
    high_S_ratio = high_S_count / total_pixels
    
    is_cartoon_like = high_S_ratio > 0.05
    
    reasons = []
    
    if is_oversaturated:
        reasons.append(f"üåà –í–∏—Å–æ–∫–∞ —Å–µ—Ä–µ–¥–Ω—è –Ω–∞—Å–∏—á–µ–Ω—ñ—Å—Ç—å ({mean_S:.2f}) ‚Äî —Ç–∏–ø–æ–≤–æ –¥–ª—è –¥–µ—è–∫–∏—Ö –®–Ü")
        
    if is_cartoon_like:
        reasons.append(f"üé® –ë–∞–≥–∞—Ç–æ '—á–∏—Å—Ç–∏—Ö' –∫–æ–ª—å–æ—Ä—ñ–≤ ({high_S_ratio:.2%}) ‚Äî –º–æ–∂–µ –≤–∫–∞–∑—É–≤–∞—Ç–∏ –Ω–∞ —Å–∏–Ω—Ç–µ—Ç–∏—á–Ω—É/–º—É–ª—å—Ç—è—à–Ω—É –≥–∞–º—É")

    ai_like = is_oversaturated or is_cartoon_like
    
    return ai_like, reasons, {"mean_s": float(mean_S), "high_s_ratio": float(high_S_ratio)}


# --- –ï–Ω–¥–ø–æ—ñ–Ω—Ç–∏ FastAPI (–±–µ–∑ –∑–º—ñ–Ω) ---

@app.get("/health")
def health():
    return {"status": "ok"}

@app.post("/analyze/image", response_model=AnalyzeResponse)
async def analyze_image(file: UploadFile = File(...)):
    raw = await file.read()
    if not raw:
        raise HTTPException(400, "–ü–æ—Ä–æ–∂–Ω—ñ–π —Ñ–∞–π–ª")

    explanations = []
    checks = {}

    # 1) EXIF/XMP
    has_exif, ai_tool_found_exif, hints = read_exif_hints(raw)
    explanations += hints
    checks["has_exif"] = has_exif
    checks["ai_tool_found_exif"] = ai_tool_found_exif
    
    # 2) –í—ñ–¥–∫—Ä–∏–≤–∞—î–º–æ –∑–æ–±—Ä–∞–∂–µ–Ω–Ω—è
    try:
        img = load_image(raw)
    except Exception:
        explanations.append("‚ùå –ù–µ –≤–¥–∞–ª–æ—Å—è –≤—ñ–¥–∫—Ä–∏—Ç–∏ –∑–æ–±—Ä–∞–∂–µ–Ω–Ω—è")
        return AnalyzeResponse(prob_ai=0.8, explanations=explanations, checks={"open_error": True})

    fmt = (img.format or "").upper()
    checks["format"] = fmt

    # 3) PNG –º–µ—Ç–∞–¥–∞–Ω—ñ
    ai_found_png, png_reasons = png_metadata_check(img, fmt)
    checks["ai_found_png_metadata"] = ai_found_png
    explanations += png_reasons
    
    # 4) –†–æ–∑–º—ñ—Ä–∏ —Ç–∞ –ø—Ä–æ–ø–æ—Ä—Ü—ñ—ó
    size_flags, size_reasons = size_checks(img)
    checks.update(size_flags)
    explanations += size_reasons

    # 5) –ê–ª—å—Ñ–∞-–∫–∞–Ω–∞–ª
    weird_alpha, alpha_reason = alpha_channel_weird(img, fmt)
    checks["weird_alpha"] = weird_alpha
    if alpha_reason:
        explanations.append(alpha_reason)

    # 6) –ü—Ä–æ—Å—Ç–∞ —á–∞—Å—Ç–æ—Ç–Ω–∞ –µ–≤—Ä–∏—Å—Ç–∏–∫–∞ (–ø–ª–∞—Å—Ç–º–∞—Å–æ–≤—ñ—Å—Ç—å)
    ai_like_freq, freq_expl, freq_vals = high_freq_heuristic(img)
    checks["ai_like_freq"] = ai_like_freq
    checks["freq"] = freq_vals
    explanations.append(freq_expl)

    # 7) JPEG quantization
    q_hint, q_reason = jpeg_quant_hint(img, fmt)
    checks["jpeg_quant_weird"] = q_hint
    if q_reason:
        explanations.append(q_reason)

    # 8) –ê—Ä—Ç–µ—Ñ–∞–∫—Ç–∏ JPEG (–µ–≤—Ä–∏—Å—Ç–∏–∫–∞)
    low_artifact_hint, artifact_reason = jpeg_artifact_hint(img, fmt)
    checks["low_artifact_hint"] = low_artifact_hint
    if artifact_reason:
        explanations.append(artifact_reason)
        
    # 9) –ê–Ω–∞–ª—ñ–∑ —à—É–º—É (NEW - –æ–Ω–æ–≤–ª–µ–Ω–æ)
    ai_too_smooth, noise_expl, noise_vals = noise_analysis(img)
    checks["ai_too_smooth"] = ai_too_smooth
    checks["noise"] = noise_vals
    explanations.append(noise_expl)
    
    # 10) –ê–Ω–∞–ª—ñ–∑ –∫–æ–ª—ñ—Ä–Ω–æ—ó —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
    ai_color_weird, color_reasons, color_vals = color_statistic_check(img)
    checks["ai_color_weird"] = ai_color_weird
    checks["color_stats"] = color_vals
    explanations += color_reasons
    
    # ---- –ó–≤–∞–∂—É–≤–∞–Ω–Ω—è –æ–∑–Ω–∞–∫ (–±–µ–∑ –∑–º—ñ–Ω) ----
    score = 0.0
    
    if ai_tool_found_exif or ai_found_png: score += 0.40
    if not has_exif and not ai_found_png and fmt == "JPEG": score += 0.15
    if size_flags["mul64"]:               score += 0.20
    if size_flags["square_common"]:       score += 0.15
    if weird_alpha:                       score += 0.10
    if ai_like_freq:                      score += 0.20
    if low_artifact_hint:                 score += 0.10
    if ai_too_smooth:                     score += 0.15
    if ai_color_weird:                    score += 0.10
    if size_flags["is_uncommon_ratio"]:   score += 0.05
    if q_hint:                            score += 0.05
    
    prob_ai = float(max(0.0, min(1.0, score)))

    w, h = size_flags["size"]
    max_dim = max(w, h)
    
    if max_dim < 384:
        prob_ai = min(1.0, prob_ai + 0.05)

    if max_dim > 2048 and not ai_tool_found_exif:
        prob_ai = max(0.0, prob_ai - 0.10) 


    # –ü—Ä–æ—Ö–æ–¥–∏–º–æ—Å—è –ø–æ —Å–ª–æ–≤–Ω–∏–∫—É checks —Ç–∞ –∫–æ–Ω–≤–µ—Ä—Ç—É—î–º–æ –≤—Å—ñ –±—É–ª–µ–≤—ñ/—á–∏—Å–ª–æ–≤—ñ —Ç–∏–ø–∏ NumPy
    final_checks = {}
    for k, v in checks.items():
        if isinstance(v, np.bool_):
            final_checks[k] = bool(v)
        elif isinstance(v, (np.float32, np.float64)):
            final_checks[k] = float(v)
        elif isinstance(v, dict):
             # –†–µ–∫—É—Ä—Å–∏–≤–Ω–æ –∫–æ–Ω–≤–µ—Ä—Ç—É—î–º–æ –≤–∫–ª–∞–¥–µ–Ω—ñ —Å–ª–æ–≤–Ω–∏–∫–∏ (–Ω–∞–ø—Ä–∏–∫–ª–∞–¥, checks["freq"])
            sub_dict = {}
            for sub_k, sub_v in v.items():
                if isinstance(sub_v, (np.float32, np.float64)):
                    sub_dict[sub_k] = float(sub_v)
                else:
                    sub_dict[sub_k] = sub_v
            final_checks[k] = sub_dict
        else:
            final_checks[k] = v

    return AnalyzeResponse(
        prob_ai=round(prob_ai, 2),
        explanations=explanations or ["‚úÖ –û–∑–Ω–∞–∫ –®–Ü –Ω–µ –≤–∏—è–≤–ª–µ–Ω–æ —è–≤–Ω–∏—Ö (–∑–æ–±—Ä–∞–∂–µ–Ω–Ω—è –≤–∏–≥–ª—è–¥–∞—î —è–∫ –∑–≤–∏—á–∞–π–Ω–µ —Ñ–æ—Ç–æ)"],
        checks=final_checks # ‚¨ÖÔ∏è –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ –æ—á–∏—â–µ–Ω–∏–π —Å–ª–æ–≤–Ω–∏–∫
    )